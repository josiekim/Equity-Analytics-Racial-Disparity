[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\nlibrary(here)\n\nhere() starts at /Users/josephinekim/Downloads/ma415 lab0 clone/final-project-team12\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(readr)\n\n&gt; library(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n&gt; library(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\n\n\n&gt; library(broom)\n\n&gt; library(dplyr)\n\n&gt; childpop &lt;- read_csv(\"dataset/childpop_race.csv\", \n+     show_col_types = FALSE)\n\n&gt; indicators &lt;- read_csv(\"dataset/rawindicators.csv\", \n+     show_col_types = FALSE)\n\n&gt; index &lt;- read_csv(\"scripts/index/index.csv\", show_col_types = FALSE)\n\n&gt; childpop &lt;- filter(childpop, year != 2010)\n\n&gt; indicators &lt;- filter(indicators, year != 2010)\n\n&gt; index &lt;- filter(index, year != 2010)\n\n&gt; merged_df &lt;- distinct(inner_join(childpop, indicators, \n+     by = \"geoid\"))\n\n&gt; cleaned_merged_df &lt;- distinct(inner_join(merged_df, \n+     index, by = \"geoid\"))\n\n&gt; cleaned_merged_df &lt;- select(cleaned_merged_df, -year, \n+     -in100, -statefips, -stateusps, -pop, -year.y, -msaname15.y, \n+     -in100.y, -countyfi .... [TRUNCATED] \n\n&gt; cleaned_merged_df &lt;- cleaned_merged_df %&gt;% rename(year = year.x, \n+     in100 = in100.x, msaid15 = msaid15.x, msaname15 = msaname15.x, \n+     county .... [TRUNCATED] \n\n&gt; na_rows_count &lt;- sum(!complete.cases(cleaned_merged_df))\n\n&gt; cleaned_data &lt;- na.omit(cleaned_merged_df)\n\n&gt; save(cleaned_data, file = here::here(\"dataset/cleaned_data.RData\"))\n\n&gt; load(\"dataset/cleaned_data.RData\")\n\n&gt; saveRDS(cleaned_data, file = \"dataset/cleaned_data.rds\")\n\n&gt; loaded_data &lt;- readRDS(file = \"dataset/cleaned_data.rds\")\n\n\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) which appears as cleaning script."
  },
  {
    "objectID": "data.html#data-background",
    "href": "data.html#data-background",
    "title": "Data",
    "section": "Data Background",
    "text": "Data Background\nLink: https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database\nThe Child Opportunity Index 2.0 (COI 2.0) is a comprehensive tool developed in collaboration with the Kirwan Institute for the Study of Race and Ethnicity at Ohio State University, released in January 2020. It measures neighborhood-level resources and conditions essential for children’s healthy development across the United States. The COI is a composite index of children’s neighborhood opportunity that contains data for every neighborhood (census tract) in the United States from every year for 2012 through 2021. It is comprised of 44 indicators in three domains (education, health and environment, and social and economic) and 14 subdomains. COI 2.0 compiles data from state and federal sources. This comprehensive data collection process examines approximately 72,000 neighborhood conditions crucial for children’s well-being, including school quality, green space availability, air quality, and adult employment levels. The resulting Child Opportunity Score, ranging from 1 to 100, provides insight into each U.S. neighborhood and metro area’s position in the national child opportunity landscape, aiming to spotlight disparities within the largest 100 U.S. metros. Particularly, it reveals significant racial inequities with implications for children’s health, education, and life expectancy.\nLink: https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/44ee1ea6-a3df-4af3-93b7-c4695d5ab6a6\nThe component indicators data for COI 2.0 is a comprehensive collection of demographic data and raw indicator values from the American Community Survey (ACS) covering the years 2010 and 2015, correlating to the ACS 5-year data from 2008-12 and 2013-17 respectively. Most of the data points related to geographical and demographic information are structured to be similar to our original data set. Notably, this data set also includes raw numeric values for educational and socio-economic data such as college enrollment in nearby institutions (’ED_COLLEGE’) and poverty rate(’SE_POVRATE’).\nThis dataset serves as a critical tool for understanding demographic distributions and trends, which will aid our analysis of the Child Opportunity Index.\nLink: https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/44ee1ea6-a3df-4af3-93b7-c4695d5ab6a6\nThe “Child population data - Number of children aged 0-17 years by race/ethnicity from the American Community Survey” uses Census data to describe the demographic of a micro regional area, including whether the area is one of the 100 largest metro areas in the U.S. The dataset includes the racial composition of the area broken down by proportions of children belonging to each race group.\nWe will use this dataset to help inform our analysis of the strength of select indicators in computing the overall Child Opportunity Index.\nTo add more insight on the data, we will be using 3 different data files under the COI 2.0, which are the index data, child population data, and component indicators data respectively. There are 8 variables that are present in all 3 data sets, which, in essence, represent the geographical location of each metropolitan area. Some important variables to highlight in these datasets are the population of each race group per metro area, health insurance coverage, poverty rate, and median household income. We will keep a close eye on these features to see how they affect a child’s overall opportunity."
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nTo clean our data set, we decided to remove columns “c5_ED_met”, “c5_HE_met”, “c5_SE_met”, “c5_COI_met”, “r_ED_met”, “r_HE_met”, “r_SE_met”, “r_COI_met”. These columns in particular contained majority empty cells and we discovered that they did not contribute much to the overall analyses of our data set. We added 3 dummy variables to indicate whether or not the census tract is within one of the 100 largest metro areas, within a micro/metro area that’s not within one of the 100, and if it’s not within a metro/micro area. If it it’s True, the value in the column will be 1.\nIn the next step of our data cleaning process, we used read.csv to access our 3 data sets childpop_race.csv, rawindicators.csv, and index.csv. Then, in order to narrow the scope of our analysis, we used filter(year != 2010) to isolate data from the year 2010. We decided to merge all 3 data sets on geoid using inner_join since it was present in all 3 data sets.\n\nmerged_df &lt;- inner_join(childpop, indicators, by = \"geoid\") |&gt;\n  distinct()\ncleaned_merged_df &lt;- inner_join(merged_df, index, by = \"geoid\") |&gt;\n  distinct()\n\nUpon merging all 3 data sets, we had an abundance of duplicated columns. We selected the duplicated columns and placed a negative sign, -, before the column name to indicate it’s being removed. Additionally, we renamed the columns using rename() to clearly show what the columns are representing. The original column names were difficult to interpret and we found ourselves referring to the data dictionary often. In the last step of our data cleaning process, we removed any null values in the data. Since these null values were sparse, we chose not to remove the columns themselves as we had done in the beginning. Instead, we removed the rows that contained the null values by counting and summing the null rows and then omitting them with na.omit(). We also grouped the different states (including Washington D.C.) by Northwest, Northeast, South, and West regions into a new data frame called region_data. Additionally, we grouped the different racial groups by state to get a total of the different race populations per state to better visualize the distribution of race populations per state.\nIn our load_and_clean_data.R file, the code snippet utilizes the tidyverse package to handle data manipulation and analysis tasks efficiently. It reads in a RData file named cleaned_merged_dg.RData located within the datasets directory using the load() function. The cleaned data is saved as an .rds file named cleaned_merged_df.rds in the scripts directory using the saveRDS function. This ensures that the cleaned data set can be easily accessed and shared for future analyses. Additionally, the readRDS function is used to read the saved .rds file, enabling the retrieval of the cleaned data set for further processing or exploration."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering? # how does child’s housing environment (food access, greenspace access, heat exposure, ozone concentration, airborne microparticles, walkability, waste dump sites, industrial pollutants) impact child’s success (z_HE_nat) # how does child’s social environment (housing vacancy rate, poverty rate, public assistant rate, home ownership rate, high skill employment, med house income, employment rate) impact z_SE_nat # how does child’s education environment (AP enrollment, adult education attainment, college enrollment, early child enrollment, HS grad, math prof, reading prof, school pov, teacher_exp, edu centers, high quality edu centers) impact z_ED_RATE\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework. # linear reg: poverty vs COI z score # logistic reg NEEDED\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.) # can use intuition and info from visualizations\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 7\n\n\nThesis\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6\n\n\nContinuation of exploratory data analysis\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 5\n\n\nDescription of data set 2\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4\n\n\nTrends in the Data\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3\n\n\nData For Equity\n\n\nHow principles for advancing equitable data practice are relevant to our data\n\n\n\n\n\nApr 1, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2\n\n\nData Background & Data Loading and Cleaning\n\n\nDeeper understanding of dataset & summary and visual of cleaned data\n\n\n\n\n\nMar 18, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1\n\n\n3 Datasets\n\n\nSummary of 3 datasets\n\n\n\n\n\nFeb 28, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-04-29-blog-7/blog-7.html",
    "href": "posts/2024-04-29-blog-7/blog-7.html",
    "title": "Blog 7",
    "section": "",
    "text": "Tentative Thesis: The opportunities available to children are determined by various socioeconomic, health and environment, and education factors, especially employment rate, health insurance, and adult education attainment.\nWe have created 3 separate regression models, one for socioeconomic, health and environment, and education. These regression models show the t-statistics and p-values for all the response variables.\nWe decided to base the statistical significance on the t-statistic since there is overlap with the p-values. It was easiest for us to determine the which response variable was most significant by looking at which absolute value was the highest. SE employment rate had a t-statistic of 166.206, HE health insurance had one of 86.873, and ED adult education attainment’s was 106.482. We see that the t-statistics for employment rate, adult education attainment, and health insurance are the highest, which aligns with our thesis.\nAdditionally, we created histograms of the residuals for each of the regression models to visualize their distributions. All 3 histograms show a normal distribution. These are tentative graphics that as of now, simply show the information. We plan on using ggpubr to improve the aesthetics of our graphs and make them easier to understand. Besides the overall looks of our graphics, we plan on writing analyses for each one to highlight the areas we want to focus on and to provide an explanation of the results and how it relates to the project. We were unable to upload the regression models and histograms, which is something we will fix for the future."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-04-08-blog-4/blog-4.html",
    "href": "posts/2024-04-08-blog-4/blog-4.html",
    "title": "Blog 4",
    "section": "",
    "text": "As a team, we have decided to get a more detailed explanation for the trend by exploring its relationship to other variables. In Blog 2, we looked at County and Weighted Average of Social and Economic Domain Component. But to get a better understanding of how County affects the child opportunity levels, we plan to explore the relationship between Education Domain and Health and Environment Domain. This is because by looking at a singular component of the data, we only get narrow understanding of how location can impact opportunity levels. This factors into the depth aspect of data exploration. By focusing on specific areas, we can see how these variables fit and whether other variables should be incorporated.\nWe’ve chosen bar plots to examine the relationship between our area variables (since they are dummy variables), as they effectively illustrate the frequency distribution of each category. Bar plots are particularly useful for comparing the occurrence of different categories, aiding in our understanding of the data’s relationships. Additionally, we will conduct linear regressions for the variables to analyze the relationship between the dependent variable and one or more independent variables by fitting a linear equation to the observed data. For our response variable, we will look at the opportunity levels within the different domains. Our predictor variables will be the different areas (in100).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nindex &lt;- read_csv('scripts/index/index.csv')\n\nRows: 144408 Columns: 37\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): geoid, msaname15, countyfips, statefips, stateusps, c5_ED_nat, c5_...\ndbl (20): year, in100, msaid15, pop, z_ED_nat, z_HE_nat, z_SE_nat, z_COI_nat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlm_model &lt;- lm(r_HE_nat ~ in100, data = index)\nsummary(lm_model)\n\n\nCall:\nlm(formula = r_HE_nat ~ in100, data = index)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.167 -26.451  -2.451  25.549  55.549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.1668     0.1469 307.470  &lt; 2e-16 ***\nin100        -0.7162     0.1757  -4.076 4.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.48 on 133805 degrees of freedom\n  (10601 observations deleted due to missingness)\nMultiple R-squared:  0.0001242, Adjusted R-squared:  0.0001167 \nF-statistic: 16.62 on 1 and 133805 DF,  p-value: 4.575e-05\n\n\n\nlibrary(ggplot2)\n\nggplot(index, aes(x = in100)) +\n  geom_bar() +\n  labs(\n    title = \"Distribution of in100\",\n    x = \"in100\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 10601 rows containing non-finite outside the scale range\n(`stat_count()`)."
  },
  {
    "objectID": "posts/2024-02-28-blog-1/blog-1.html",
    "href": "posts/2024-02-28-blog-1/blog-1.html",
    "title": "Blog 1",
    "section": "",
    "text": "#Data set 1:https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/080cfe52-90aa-4925-beaa-90efb04ab7fb\nThe Child Opportunity Index 2.0 database measures neighborhood resources and conditions affecting children’s development and access to opportunities. They used census data from 2015. (Indexes based on the 2010 census are also available.) We will be able to clean the data (though it seems quite clean already). There are 144,408 entries/rows. To better understand the racial demographics of these zip codes, we will need to find the zip codes with the lowest opportunity scores and independently find the racial demographics of these zip codes. Race data is not included in this data set. A question we hope to explore is which of the 29 indicators in the Child Opportunity Index 2.0 database was the greatest predictor of opportunity score? If not measuring based only on one of the indicators, which of the three domains these indicators fall within (education, health and environment, and social and economic) is the best predictor of overall score? Another question we are interested in is based on the 10 zip codes with the lowest composite opportunity scores, and the 10 highest. How do the racial demographics of these two groups differ?\n#Data set 2: https://www.kaggle.com/datasets/rishidamarla/arrests-by-race\n“Arrests by Race” has 19 columns and 31 rows. The rows show the cause for arrest, some being more violent than others. The columns show the total of different racial groups. The data was collected to gain an understanding on number of persons arrested nationwide in 2018 broken down by race and ethnicity. We would be able to load and clean (if needed further cleaning) the dataset to analyze the data. This dataset only shows people arrested by law enforcement that provided their racial demographics. Therefore, it is important to note that the totals for each race may be different than the ethnicity totals. In addition, the data only represents the person arrested and not how many times that person was arrested. A question we hope to explore is are minority races charged with offenses more than White people? Another question is are Black people more likely to get arrested than White people for loitering (or also suspicion) crimes?\n#Data set 3:https://www.census.gov/programs-surveys/ces/data/public-use-data/race-and-economic-opportunity-data-tables.html\nThe data comes from race and economic opportunity. Most previous work on racial disparities has studied inequality within a single generation of people. However, this datasets illustrate the racial gaps among generations. Using de-identified data covering 20 million children and their parents, the data shows how race currently shapes opportunity in the U.S. and how we can reduce racial disparities going forward. Specially, this data illustrates National Statistics by Parent Income Percentile, Gender, and Race. For the analysis of this data, it is necessary for us to interpret and understand the variables. There are 69 variables and we need to identify the dummy variables like gender. For different race, such as Asian, Black, hisp etc. Those data are ranked by income rank. With various kinds of combination, the data can be very messy. We are thinking about filter out the gender and emphasize the impact of different races. There are different outcomes of those children. Going to college, get married or getting in to prison. I believe those are categorical variable and maybe we can convert them into numerical variables to see if there is a correlation between children’s outcome and parents’ incomes and races."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "MA [46]15 Final Project - TEAM 12",
    "section": "",
    "text": "library(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\ncleaned_data &lt;- readRDS('dataset/cleaned_data.rds')\n\nunique_states &lt;- unique(cleaned_data$state_usps)\nlength(unique_states)\n\n[1] 43\n\nregion_data &lt;- data.frame(\n  state_usps = c(\n    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \n    \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n    \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \n    \"WI\", \"WY\", \"DC\"\n  ),\n  region = c(\n    \"South\", \"West\", \"West\", \"South\", \"West\", \"West\", \"Northeast\", \"Northeast\", \"South\", \"South\", \n    \"West\", \"West\", \"Midwest\", \"Midwest\", \"Midwest\", \"Midwest\", \"South\", \"South\", \"Northeast\", \n    \"Northeast\", \"Northeast\", \"Midwest\", \"Midwest\", \"South\", \"Midwest\", \"West\", \"Midwest\", \"Midwest\", \n    \"West\", \"Northeast\", \"Northeast\", \"West\", \"South\", \"Midwest\", \"Midwest\", \"South\", \"Northeast\", \n    \"South\", \"Midwest\", \"Northeast\", \"South\", \"Midwest\", \"South\", \"West\", \"West\", \"Northeast\", \"South\",\n    \"West\", \"Northeast\", \"Northeast\", \"Northeast\"\n  )\n)\n\ncollege_data &lt;- cleaned_data %&gt;%\n  left_join(region_data, by = \"state_usps\") %&gt;%\n  select(aian, api, black, white, hisp, college_enrollment, state_usps, region)\n\ncollege_data_long &lt;- tidyr::pivot_longer(college_data,\n                                         cols = c(aian, api, black, white, hisp),\n                                         names_to = \"race\",\n                                         names_repair = \"unique\")\n\ncollege_data_summarized &lt;- college_data_long %&gt;%\n  group_by(region, race) %&gt;%\n  summarize(average_college_enrollment = mean(value, na.rm = TRUE))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\nggplot(college_data_summarized, aes(x = race, y = average_college_enrollment, fill = race)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~region) +  # Facet by region\n  labs(title = \"Average College Enrollment by Race and Region\",\n       x = \"Race\",\n       y = \"Average College Enrollment\",\n       fill = \"Race\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-04-18-blog-6/blog-6.html",
    "href": "posts/2024-04-18-blog-6/blog-6.html",
    "title": "Blog 6",
    "section": "",
    "text": "We decided to explore the college enrollment of races by state to further our exploratory data analysis. Since we are analyzing opportunity levels by race for different regions, we thought college enrollment would be a good indicator. This is because college enrollment provides a clear understanding of the earlier education opportunities the children had. For example, if the children were enrolled into colleges, they likely had access to education centers or were enrolled in AP classes. We also are able to assume that they graduated from high school or had proof of a GED. From our graph, we see that White students had the highest college enrollment across the 4 regions, towering over the other race groups (except for Hispanic students in the West). This shows that White students had higher opportunities over other students, but considering this is early in their adult life and only one indicator of opportunity levels, we also wanted to explore their home ownership rate. We found that by graphing the home ownership rate, White students surpassed the other races. We thought home ownership would further our analysis and understanding since it’s a strong measure of how one is doing in life. It shows that the student has a stable income, possibly a family, and is looking to settle down. We plan to repeat this process for various other variables such as median income.\nWe will continue graphing, but we also want to begin creating prediction models. We are curious as to what factors go into college enrollment and the significance of them. For this reason, we will be running regression models to predict college enrollment with independent variables such as median income, AP class enrollment, and access to education centers. Based on the significance of the variables, we will be delving deeper into which race groups and regions have the highest opportunity levels for the significant factors. As of now, this process will be repeated for employment rate and home ownership rate as well. However, based on our analysis, we are flexible to predict other variables using the relevant independent variables."
  },
  {
    "objectID": "posts/2024-04-01-blog-3/blog-3.html",
    "href": "posts/2024-04-01-blog-3/blog-3.html",
    "title": "Blog 3",
    "section": "",
    "text": "Transparency: Be transparent about the limits of the data\nA limitation of data is making analyses that the data does not support. In order to mitigate any unsupported analyses, it is important to be transparent about what data is being used and the process of arriving at the conclusion. Transparency is a relevant limitation to our data set since transparency is a necessity and missing quality in all data sets. One potential for abuse or misuse of our data set is how the data was collected. The data set mainly aims to understand the opportunity levels of children within the 100 largest metros in the US. For this reason, it is possible to make a conclusion that children outside the largest metros have lower health and environment opportunities. However, this would be an unsupported analysis since there is not enough data in the data set to confidently make that conclusion. Another limitation of the data is the z-score measurements. It is unclear what the exact measurements went into calculating the z-score. By not knowing the process of calculating the z-scores, it creates ambiguity and uncertainty when analyzing the score. Urban Institute, as mentioned earlier in this paragraph, emphasizes the importance of being transparent throughout the data collection and analysis process for the reason that it shows validity and credibility. Additionally, by adhering to the practice of being transparent, we can reduce the number of unreliable analyses and more accurately relay our findings to our audience.\n\n\nData Collection: Avoid undue burden\nEnsuring justice in the data collection process is crucial for any data collection. According to the guiding principles for data collection, data collection should reflect respect for the rights of individuals and community groups and minimize undue burden—in other words, eliminate the collection of nonessential elements. Implementing justice in the collection would require a thorough review of the data collection methodology to identify and eliminate unnecessary burdens. It requires regularly reviewing the necessity and usage of collected data elements, ensuring that only relevant data are retained and used. While avoiding undue burdens for the individuals and communities being surveyed is an ethical method for data collection, this also helps maintain cleaner data since it is constantly being checked, and irrelevant data will not be included. Although it is important to adhere to this principle, there are potential limitations to avoiding collecting information deemed too sensitive for certain individuals or groups. It might sound counter-intuitive as the principle asks to prevent unnecessary data collection, but one cannot be entirely sure what data is irrelevant before doing the analysis. Adhering to the justice principle could lead to limitations such as data availability, the potential for certain neighborhoods to be misrepresented, or the index’s inability to capture every nuance of child development environments. The principle itself is an action to reduce abuse and misuse of the data; adhering to this principle would entail not only careful presentation and interpretation of the data but also guidance on its ethical use to inform policy and intervention without reinforcing negative stereotypes or inequalities.\n\n\nAccessability: Return data and research results to community members in a form they can use\nEnsuring that our findings are accessible to people whose data we’ve analyzed is ethically important. After performing our data analysis and finding trends in the data, if valuable, we might share our findings with the creators of the Child Opportunity Index (COI), the Kirwan Institute for the Study of Race and Ethnicity at Ohio State University. This information may also be valuable to nonprofits. Sharing our findings with local governments may also be important, so that assistance programs may be able to specifically target areas with low CO indexes. It would be crucial to publish findings on public or free platforms to promote accessibility."
  },
  {
    "objectID": "posts/2024-03-18-blog-2/blog-2.html",
    "href": "posts/2024-03-18-blog-2/blog-2.html",
    "title": "Blog 2",
    "section": "",
    "text": "Data Background\nThe Child Opportunity Index 2.0 (COI 2.0) is a tool developed by researchers at Brandeis University. The data for COI 2.0 is collected from various state and federal sources, combining information on 29 indicators across three domains: education, health and environment, and social and economic. This meticulous data collection and analysis process is aimed at providing a comprehensive picture of the conditions affecting children’s healthy development across the United States. The COI 2.0 builds on the first version of the Index, created in 2014, and was updated and improved, launching on January 22, 2020. It’s designed to capture a wide range of approximately 72,000 neighborhood conditions like the quality of schools, availability of green spaces, air pollution levels, and the number of adults with high-skill jobs, which are vital for children’s health and development. This tool ranks every U.S. neighborhood and metro area by assigning a Child Opportunity Score from 1 to 100, based on its percentile in the national child opportunity distribution.\nA key goal of the COI 2.0 is to illuminate inequities in neighborhood opportunity within the 100 largest metros in the U.S., and indeed, its first analysis revealed significant racial inequities in neighborhood conditions, which can profoundly impact the health, education, economic prospects, and life expectancy of children. For instance, it found that a majority of black children in Cleveland grow up in very-low and low-opportunity neighborhoods, illustrating a stark pattern of inequity that is replicated across many cities in the nation. The COI 2.0 has been used by researchers, city planners, community leaders, and policymakers to identify current inequities and to create strategies for addressing these disparities. For example, Albany, N.Y. utilized the Index to develop a capital improvement plan to increase access to parks and playgrounds in neglected neighborhoods predominantly inhabited by black children. Similarly, the City of Chicago incorporated the COI into its five-year strategic health plan, Healthy Chicago 2.0, to address child health inequities across the city’s neighborhoods.\n\n\nData Loading and Cleaning\nThe “Child Opportunity Levels, Scores and averaged z-scores for the overall index and three domains” data set is 33mb. For this reason, we did not need to start with a subset of the data. We decided to start the cleaning process by getting rid of the “c5_ED_met”, “c5_HE_met”, “c5_SE_met”, “c5_COI_met”, “r_ED_met”, “r_HE_met”, “r_SE_met”, “r_COI_met”. This is because the majority of values in these columns were left blank. Column “in100” also contains blanks. But after looking at the provided Data Dictionary, we saw that the blanks in this columns are assigned a value and represent whether census tract is located outside a metro or micropolitan area.\nWe decided to plot column z_SE_nat (the weighted average of social and economic domain component) since by summarizing the z_SE_nat, z_ED_nat, z_HE_nat, we saw that z_SE_nat had an abnormal range. The other two columns have a range less than 1, whereas z_SE_nat has a range greater than 1. Upon visualizing z_SE_nat, we also saw that there are a many outliers in the data. While most of the points lie within -1 to 1, there are many that lie outside of that pattern. We will be looking into the relationship between z_SE_nat and countyfips to see if there is any reasoning for the outliers, based on location.\n\nindex &lt;- readRDS('scripts/cleaned_index_dataset.rds')\n\nsummary(index$z_SE_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.806428 -0.123942  0.021308 -0.007073  0.139528  1.888642 \n\nsummary(index$z_ED_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.280814 -0.043922 -0.008535 -0.001110  0.037489  0.256106 \n\nsummary(index$z_HE_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.451968 -0.020446  0.012679  0.006041  0.039472  0.201493 \n\nsuppressPackageStartupMessages(library(tidyverse))\n\nindex %&gt;%\n  ggplot(aes(x = countyfips, y = z_SE_nat)) + \n  geom_point() +\n  labs(\n    title = \"County and Weighted Average of Social and Economic Domain Component\",\n    x = \"County\",\n    y = \"Weighted Average of Social and Economic Domain Component\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-04-12-blog-5/blog-5.html",
    "href": "posts/2024-04-12-blog-5/blog-5.html",
    "title": "Blog 5",
    "section": "",
    "text": "https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/44ee1ea6-a3df-4af3-93b7-c4695d5ab6a6\nhttps://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/f16fff12-b1e5-4f60-85d3-3a0ededa30a0\nThese will be the two datasets that we will be combining to our original dataset. The first dataset has information about the demographic population in each metropolitan area based on a “geoid”. The second dataset has information about features that play into a child’s overall opportunity, such as whether their parents attended college, whether there is access to healthy foods, and etc, in each metropolitan area. By combining these two datasets with the original one, we can better understand what features contribute to a child’s overall opportunity and further explore how certain features in a metropolitan area affect certain demographics of children. In addition to this, these two datasets allow us to better characterize neighborhoods and understand the disparities and advantages that exist within them.\nWe are still in the middle of figuring out how to combine our datasets together, but we have the general idea that all three datasets have the 8 same variables: ‘geoid’, ‘year’, ‘in100’, ‘msaid15’, ‘msaname15’, ‘countyfips’, ‘statefips’, and ‘stateusps’. Knowing this, we will plan to join these datasets on the ‘geoid’ since this variable is unique for each metropolitan area, leading to a smooth combination. Prior to performing the combination, we will ensure that the data types are the same across all datasets so that the merged dataset remains accurate."
  },
  {
    "objectID": "dataset/testing.html",
    "href": "dataset/testing.html",
    "title": "MA [46]15 Final Project - TEAM 12",
    "section": "",
    "text": "library(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(broom)\n\nchildpop &lt;- read_csv('dataset/childpop_race.csv', show_col_types = FALSE)\nindicators &lt;- read_csv('dataset/rawindicators.csv', show_col_types = FALSE)\nindex &lt;- read_csv('scripts/index/index.csv', show_col_types = FALSE)\n\n\nchildpop &lt;- childpop |&gt;\n  filter(year != 2010)\n\nindicators &lt;- indicators |&gt;\n  filter(year != 2010)\n\nindex &lt;- index |&gt;\n  filter(year != 2010)\n\n\nmerged_df &lt;- inner_join(childpop, indicators, by = \"geoid\") |&gt;\n  distinct()\ncleaned_merged_df &lt;- inner_join(merged_df, index, by = \"geoid\") |&gt;\n  distinct()\n\n\nlibrary(dplyr)\ncleaned_merged_df &lt;- select(cleaned_merged_df, -year, -in100, -statefips, -stateusps, -pop, -year.y, -msaname15.y, -in100.y, -countyfips.y, -statefips.y, -pop.y, -msaid15.y, -stateusps.y, -countyfips, -pop.x, -msaid15, -msaname15)\n\n\ncleaned_merged_df &lt;- cleaned_merged_df %&gt;%\n  rename(\n    year = year.x,\n    in100 = in100.x,\n    msaid15 = msaid15.x,\n    msaname15 = msaname15.x,\n    county_fips = countyfips.x,\n    state_fips = statefips.x,\n    state_usps = stateusps.x,\n    total_pop = total,\n    AP_enrollment = ED_APENR,\n    adult_edu_attainment = ED_ATTAIN,\n    college_enrollment = ED_COLLEGE,\n    early_child_enrollment = ED_ECENROL,\n    HS_gradrate = ED_HSGRAD,\n    math_prof = ED_MATH,\n    reading_prof = ED_READING,\n    school_pov = ED_SCHPOV,\n    teacher_exp = ED_TEACHXP,\n    edu_centers = ED_PRXECE,\n    high_quality_edu_centers = ED_PRXHQECE,\n    food_access = HE_FOOD,\n    green_space_access = HE_GREEN,\n    heat_exposure = HE_HEAT,\n    health_insurance = HE_HLTHINS,\n    ozone_conc = HE_OZONE,\n    airborne_microparticles = HE_PM25,\n    housing_vac_rate = HE_VACANCY,\n    walkability = HE_WALK,\n    waste_dump_sites = HE_SUPRFND,\n    indus_pollutants = HE_RSEI,\n    poverty_rate = SE_POVRATE,\n    pub_assist_rate = SE_PUBLIC,\n    home_ownership_rate = SE_HOME,\n    high_skill_employ = SE_OCC,\n    med_house_income = SE_MHE,\n    employ_rate = SE_EMPRAT,\n    commute_dur = SE_JOBPROX,\n    single_household = SE_SINGLE\n  )\n\n\nna_rows_count &lt;- sum(!complete.cases(cleaned_merged_df))\ncleaned_data &lt;- na.omit(cleaned_merged_df)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#josephine-kim",
    "href": "about.html#josephine-kim",
    "title": "About",
    "section": "Josephine Kim",
    "text": "Josephine Kim\nJosephine is a junior studying Data Science at Boston University."
  },
  {
    "objectID": "about.html#phillip-kim",
    "href": "about.html#phillip-kim",
    "title": "About",
    "section": "Phillip Kim",
    "text": "Phillip Kim\nPhillip is a senior at Boston University majoring in Data Science."
  },
  {
    "objectID": "about.html#zhisheng-yang",
    "href": "about.html#zhisheng-yang",
    "title": "About",
    "section": "Zhisheng Yang",
    "text": "Zhisheng Yang\nZhisheng is a senior studying Economics and Mathematics at Boston University."
  },
  {
    "objectID": "about.html#kajsa-kedefors",
    "href": "about.html#kajsa-kedefors",
    "title": "About",
    "section": "Kajsa Kedefors",
    "text": "Kajsa Kedefors\nKajsa is a senior at Boston University pursuing a joint major in philosophy and psychology, and an independent major in social science and public policy. She minors in journalism."
  },
  {
    "objectID": "about.html#jiahua-chang",
    "href": "about.html#jiahua-chang",
    "title": "About",
    "section": "Jiahua Chang",
    "text": "Jiahua Chang\nJiahua (Richard) is a senior at Boston University majoring in Math and Economics.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]
[
  {
    "objectID": "data.html#data-set-1-child-opportunity-index",
    "href": "data.html#data-set-1-child-opportunity-index",
    "title": "Data",
    "section": "Data Set 1: Child Opportunity Index",
    "text": "Data Set 1: Child Opportunity Index\nThe Child Opportunity Index 2.0 (COI 2.0) is a comprehensive tool developed in collaboration with the Kirwan Institute for the Study of Race and Ethnicity at Ohio State University, released in January 2020. It measures neighborhood-level resources and conditions essential for a child’s healthy development across the United States. The COI is a composite index of child’s neighborhood opportunity that contains data for every neighborhood (census tract) in the United States from every year for 2012 through 2021. It is comprised of 44 indicators in three domains (education, health and environment, and social and economic) and 14 sub-domains. COI 2.0 compiles data from state and federal sources. This comprehensive data collection process examines approximately 72,000 neighborhood conditions crucial for a child’s well-being, including school quality, green space availability, air quality, and adult employment levels. The resulting Child Opportunity Score, ranging from 1 to 100, provides insight into each U.S. neighborhood and metro area’s position in the national child opportunity landscape, aiming to spotlight disparities within the largest 100 U.S. metros. Particularly, it reveals significant racial inequities with implications for a child’s health, education, and life expectancy.\n\nSimplified Data Dictionary\n\n\n\n\n\n\n\nAbbreviation\nDescription\n\n\n\n\nED\nEducation Domain\n\n\nHE\nHealth and Environment Domain\n\n\nSE\nSocial and Economic Domain\n\n\nCOI\nOverall COI, Weighted average of three domain averaged z-scores\n\n\nz\nz-score\n\n\nc5\nChild Opportunity Levels\n\n\nr\nChild Opportunity Scores\n\n\nnat\nnationally-normed\n\n\nstt\nstate-normed\n\n\nmet\nmetro-normed"
  },
  {
    "objectID": "data.html#data-set-2-raw-indicator-values",
    "href": "data.html#data-set-2-raw-indicator-values",
    "title": "Data",
    "section": "Data Set 2: Raw Indicator Values",
    "text": "Data Set 2: Raw Indicator Values\nThe component indicators data for COI 2.0 is a comprehensive collection of demographic data and raw indicator values from the American Community Survey (ACS) covering the years 2010 and 2015, correlating to the ACS 5-year data from 2008-12 and 2013-17 respectively. Most of the data points related to geographical and demographic information are structured to be similar to our original data set. Notably, this data set also includes raw numeric values for educational and socioeconomic data such as college enrollment in nearby institutions (’ED_COLLEGE’) and poverty rate(’SE_POVRATE’).\nThis data set serves as a critical tool for understanding demographic distributions and trends, which will aid our analysis of the Child Opportunity Index."
  },
  {
    "objectID": "data.html#data-set-3-child-population-data",
    "href": "data.html#data-set-3-child-population-data",
    "title": "Data",
    "section": "Data Set 3: Child Population Data",
    "text": "Data Set 3: Child Population Data\nThe “Child population data - Number of children aged 0-17 years by race/ethnicity from the American Community Survey” uses Census data to describe the demographic of a micro regional area, including whether the area is one of the 100 largest metro areas in the U.S. The data set includes the racial composition of the area broken down by proportions of children belonging to each race group.\nWe will use this data set to help inform our analysis of the strength of select indicators in computing the overall Child Opportunity Index.\nTo add more insight on the data, we will be using 3 different data files under the COI 2.0, which are the index data, child population data, and component indicators data respectively. There are 8 variables that are present in all 3 data sets, which, in essence, represent the geographical location of each metropolitan area. Some important variables to highlight in these data sets are the population of each race group per metro area, health insurance coverage, poverty rate, and median household income. We will keep a close eye on these features to see how they affect a child’s overall opportunity.\nBelow, we created a graphics with the variable college_enrollment to show the distribution across race and region."
  },
  {
    "objectID": "data.html#average-college-enrollment-by-race-and-region",
    "href": "data.html#average-college-enrollment-by-race-and-region",
    "title": "Data",
    "section": "Average College Enrollment by Race and Region",
    "text": "Average College Enrollment by Race and Region\n\nWe decided to visualize the average college enrollment across race and region due to the growing weight having a college degree holds in society. Obtaining a college degree is necessary for a majority of careers as it shows that they are an educated and hard working person. It’s considered a threshold for success, and is a result of the opportunities children had growing up.\nAs seen by the bar graphs, White people consistently have high college enrollment status across the 4 regions. On the other hand, Aian (Native American and Alaska Native) hold the lowest numbers. The reappearing pattern of White and Hispanic students having higher college enrollment numbers compared to the other races is something we found intriguing and hope to explore further."
  },
  {
    "objectID": "data.html#data-cleaning-process",
    "href": "data.html#data-cleaning-process",
    "title": "Data",
    "section": "Data Cleaning Process",
    "text": "Data Cleaning Process\nIn the first step of our data cleaning process, we used read.csv to access our 3 data sets childpop_race.csv, rawindicators.csv, and index.csv. Then, in order to narrow the scope of our analysis, we used filter(year != 2010) to isolate data from the year 2010. We decided to merge all 3 data sets on geoid using inner_join since it was present in all 3 data sets.\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tune         1.2.1 \n✔ infer        1.0.7      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.1.0 \n✔ parsnip      1.2.1      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(broom)\n\nchildpop &lt;- read_csv('dataset/childpop_race.csv', show_col_types = FALSE)\nindicators &lt;- read_csv('dataset/rawindicators.csv', show_col_types = FALSE)\nindex &lt;- read_csv('scripts/index/index.csv', show_col_types = FALSE)\n\nchildpop &lt;- childpop |&gt;\n  filter(year != 2010)\nindicators &lt;- indicators |&gt;\n  filter(year != 2010)\nindex &lt;- index |&gt;\n  filter(year != 2010)\n\nmerged_df &lt;- inner_join(childpop, indicators, by = \"geoid\") |&gt;\n  distinct()\ncleaned_merged_df &lt;- inner_join(merged_df, index, by = \"geoid\") |&gt;\n  distinct()\n\nUpon merging all 3 data sets, we had an abundance of duplicated columns. We selected the duplicated columns and placed a negative sign, -, before the column name to indicate it’s being removed.\n\ncleaned_merged_df &lt;- select(cleaned_merged_df, -year, -in100, -statefips, -stateusps, -pop, -year.y, -msaname15.y, -in100.y, -countyfips.y, -statefips.y, -pop.y, -msaid15.y, -stateusps.y, -countyfips, -pop.x, -msaid15, -msaname15)\n\nAdditionally, we renamed the columns using rename() to clearly show what the columns are representing. The original column names were difficult to interpret and we found ourselves referring to the data dictionary often. Certain variables, such as z_COI_nat, we decided to keep as is since changing the names to describe the data would make the column names long and repetitive. Instead, we decided to include a simplified version of the data dictionary in our Data page under Data Set 1: Child Opportunity Index.\nIn the last step of our data cleaning process, we removed any null values in the data. We removed the rows that contained the null values by counting and summing the null rows and then omitting them with na.omit(). We also grouped the different states (including Washington D.C.) by Northwest, Northeast, South, and West regions into a new data frame called region_data. Additionally, we grouped the different racial groups by state to get a total of the different race populations per state to better visualize the distribution of race populations per state.\n\nna_rows_count &lt;- sum(!complete.cases(cleaned_merged_df))\ncleaned_data &lt;- na.omit(cleaned_merged_df)"
  },
  {
    "objectID": "data.html#load-data",
    "href": "data.html#load-data",
    "title": "Data",
    "section": "Load Data",
    "text": "Load Data\nIn our load_and_clean_data.R file, the code snippet utilizes the tidyverse package to handle data manipulation and analysis tasks efficiently. It reads in a RData file named cleaned_merged_dg.RData located within the datasets directory using the load() function. The cleaned data is saved as an .rds file named cleaned_merged_df.rds in the scripts directory using the saveRDS function. This ensures that the cleaned data set can be easily accessed and shared for future analyses. Additionally, the readRDS function is used to read the saved .rds file, enabling the retrieval of the cleaned data set for further processing or exploration.\n\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = TRUE # Use echo=FALSE or omit it to avoid code output  \n)\n\n\n&gt; library(readr)\n\n&gt; library(tidyverse)\n\n&gt; library(tidymodels)\n\n&gt; library(broom)\n\n&gt; library(dplyr)\n\n&gt; childpop &lt;- read_csv(\"dataset/childpop_race.csv\", \n+     show_col_types = FALSE)\n\n&gt; indicators &lt;- read_csv(\"dataset/rawindicators.csv\", \n+     show_col_types = FALSE)\n\n&gt; index &lt;- read_csv(\"scripts/index/index.csv\", show_col_types = FALSE)\n\n&gt; childpop &lt;- filter(childpop, year != 2010)\n\n&gt; indicators &lt;- filter(indicators, year != 2010)\n\n&gt; index &lt;- filter(index, year != 2010)\n\n&gt; merged_df &lt;- distinct(inner_join(childpop, indicators, \n+     by = \"geoid\"))\n\n&gt; cleaned_merged_df &lt;- distinct(inner_join(merged_df, \n+     index, by = \"geoid\"))\n\n&gt; cleaned_merged_df &lt;- select(cleaned_merged_df, -year, \n+     -in100, -statefips, -stateusps, -pop, -year.y, -msaname15.y, \n+     -in100.y, -countyfi .... [TRUNCATED] \n\n&gt; cleaned_merged_df &lt;- cleaned_merged_df %&gt;% rename(year = year.x, \n+     in100 = in100.x, msaid15 = msaid15.x, msaname15 = msaname15.x, \n+     county .... [TRUNCATED] \n\n&gt; na_rows_count &lt;- sum(!complete.cases(cleaned_merged_df))\n\n&gt; cleaned_data &lt;- na.omit(cleaned_merged_df)\n\n&gt; save(cleaned_data, file = here::here(\"dataset/cleaned_data.RData\"))\n\n&gt; load(\"dataset/cleaned_data.RData\")\n\n&gt; saveRDS(cleaned_data, file = \"dataset/cleaned_data.rds\")\n\n&gt; loaded_data &lt;- readRDS(file = \"dataset/cleaned_data.rds\")"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "We are interested in seeing how the distribution of z-scores varies across the United States. So we decided to split the cleaned data into regions rather than state by using the case_when() function. Organizing the data by region facilitates easier comparisons and provides a clearer understanding of the z_COI_nat` trends. We chose to create a bar graph of the weighted averages of the z_COI_nat data by region."
  },
  {
    "objectID": "analysis.html#key-indicators-of-se-factors-in-northeast-region",
    "href": "analysis.html#key-indicators-of-se-factors-in-northeast-region",
    "title": "Analysis",
    "section": "Key Indicators of SE Factors in Northeast Region",
    "text": "Key Indicators of SE Factors in Northeast Region"
  },
  {
    "objectID": "analysis.html#key-indicators-of-se-factors-in-south-region",
    "href": "analysis.html#key-indicators-of-se-factors-in-south-region",
    "title": "Analysis",
    "section": "Key Indicators of SE Factors in South Region",
    "text": "Key Indicators of SE Factors in South Region\n\nBased on the distributions, high skill employment is the most normal. This indicates a moderate level of high skilled employment across the region. As a result, high skilled employment does not seem to have any major impact on the z-score since regions with disproportionately low levels of high skilled employment may experience higher levels of socioeconomic challenges. A skewed distribution such as employment rate stands out as a variable that heavily impacts the z-score. Employment rate is skewed to the left. The distribution suggests that a significant proportion of the population is employed and the Northeast region has a robust economy. Although poverty rate, housing vacancy rate, and public assistant rate are skewed right shows that most areas in the Northeast have low instances of these cases. This aligns with our Distribution by Average z-score by Region table which shows Northeast with the highest scores.\nThe South region follows the same pattern as the Northeast region. From analyzing the employment rate visual, the concentration of employment could also suggest challenges for unemployed individuals. This could potentially lead to issues such as income inequality. This is evident in the right skewed distribution of the median housing income graph. It shows that most of the population earns a lower income. This could be the reason the South region has a significantly lower z-score than the Northeast.\nIn order to determine which variable has the highest impact on the dependent variable (z_COI_nat), we decided to run a regression to examine the magnitude of the coefficients. The larger the absolute value of the coefficient, the stronger the impact of that variable on the dependent variable, holding other variables constant. Looking at the coefficients, the variable employ_rate has the largest (absolute value) coefficient, with an estimate of 8.193e-04. This suggests that for every unit increase in the employment rate, the z_COI_nat increases by approximately 8.193e-04 units, assuming all other variables remain constant. Additionally, med_house_income also has a substantial coefficient (1.676e-07), indicating that for every unit increase in median household income, the z_COI_nat increases by approximately 1.676e-07 units, holding other variables constant. This confirms our interpretations of the histograms for why Northeast has the highest z-scores and why South has the lowest."
  },
  {
    "objectID": "analysis.html#linear-models-for-weighted-average-of-social-and-economic-domain-z-scores-nationally-normed",
    "href": "analysis.html#linear-models-for-weighted-average-of-social-and-economic-domain-z-scores-nationally-normed",
    "title": "Analysis",
    "section": "Linear Models for Weighted Average of Social and Economic Domain z-scores, Nationally Normed",
    "text": "Linear Models for Weighted Average of Social and Economic Domain z-scores, Nationally Normed\n\nResiduals vs Fitted Plot: This plot helps to check the assumption of linearity and homoscedasticity in a regression model. Ideally, the residuals will be randomly distributed to indicate a good model fit. Here, the residuals seem to be relatively evenly distributed around the horizontal line, but a slight fan shape suggests potential heteroscedasticity, where the variance of the residuals increases as the fitted values increase.\nNormal Q-Q Plot: This plot is used to check the normality of residuals. The points should lie approximately along the reference line if the residuals are normally distributed. In this plot, most points align well with the line except for a slight deviation at the tails. This suggests that the residuals are mostly normal but with some possible deviations from normality at the extremes.\nScale-Location Plot: Similar to the first plot, this one also checks for homoscedasticity by plotting the square roots of the standardized residuals against the fitted values. The relatively constant spread across the range of fitted values suggests that variance is fairly constant (homoscedasticity), although, like the Residuals vs Fitted plot, there appears to be a slight increase in spread for higher fitted values. Another similarity is the presence of outliers, which suggest atypical levels of heteroscedasticity for those points.\nCook’s Distance Plot: This plot shows the influence of each observation on the fitted values. Observations with a Cook’s distance much larger than others can be considered influential to the model. Here, all observations have relatively low Cook’s distances, suggesting that no single observation is overly influential. Overall, the model shows a reasonable fit with some signs of potential heteroscedasticity and slight deviations from normality at the extremes. The lack of influential outliers is a good sign for the robustness of the model."
  },
  {
    "objectID": "analysis.html#key-indicators-of-ed-factors-in-northeast-region",
    "href": "analysis.html#key-indicators-of-ed-factors-in-northeast-region",
    "title": "Analysis",
    "section": "Key Indicators of ED Factors in Northeast Region",
    "text": "Key Indicators of ED Factors in Northeast Region"
  },
  {
    "objectID": "analysis.html#key-indicators-of-ed-factors-in-south-region",
    "href": "analysis.html#key-indicators-of-ed-factors-in-south-region",
    "title": "Analysis",
    "section": "Key Indicators of ED Factors in South Region",
    "text": "Key Indicators of ED Factors in South Region"
  },
  {
    "objectID": "analysis.html#section",
    "href": "analysis.html#section",
    "title": "Analysis",
    "section": "",
    "text": "Analyzing the histograms of key education indicators for the Northeast and South regions, several patterns emerge that align with broader socioeconomic trends and disparities between these areas.\nIn the Northeast, histograms such as adult education attainment, college enrollment, and education centers suggest a robust educational infrastructure with moderate to high levels of engagement and attainment. For instance, the distribution of adult education attainment and college enrollment both exhibit a normal-like distribution, centered around moderate levels. This indicates a balanced distribution of educational levels, which likely contributes to a stronger economy and higher employment rates. The histogram for education centers, which is slightly skewed towards higher numbers, further supports the availability of educational resources. Such distributions align with a more robust economic environment where higher education levels and resource availability contribute positively to socioeconomic stability.\nConversely, the South shows a different pattern. The histograms for the same indicators display significant right skewness, especially in math proficiency and school poverty rates. These distributions suggest lower levels of achievement and higher instances of poverty within schools. The histogram for adult education attainment in the South also shows a pronounced decline as educational levels increase, indicating fewer adults achieve higher education levels. These educational challenges are mirrored in socioeconomic conditions, as reflected by lower median household incomes and a higher percentage of school-associated poverty.\nBuilding on the examination of educational indicators using histographs, we conducted a regression analysis to determine which variables significantly impact the dependent variable (z_COI_nat). Among the independent variables, the variable prop_nonwhite exhibits the most significant impact with a coefficient of -0.02019, indicating that a unit increase in the proportion of the nonwhite population is associated with a decrease of approximately 0.02019 in the z_COI_nat, holding other variables constant. Similarly, the variable school_pov has a substantial negative coefficient of -0.0002494, suggesting that an increase in the school poverty rate by one unit leads to a decrease in z_COI_nat by roughly the same magnitude. In contrast, variables such as HS_gradrate and adult_edu_attainment, with coefficients of 0.0001139 and 0.0006891 respectively, show that higher levels of educational attainment correspond to positive increments in z_COI_nat."
  },
  {
    "objectID": "analysis.html#linear-models-for-weighted-average-of-education-domain-z-scores-nationally-normed",
    "href": "analysis.html#linear-models-for-weighted-average-of-education-domain-z-scores-nationally-normed",
    "title": "Analysis",
    "section": "Linear Models for Weighted Average of Education Domain z-scores, Nationally Normed",
    "text": "Linear Models for Weighted Average of Education Domain z-scores, Nationally Normed\n\nResiduals vs Fitted Plot: This plot helps to check the assumption of linearity and homoscedasticity in a regression model. Ideally, the residuals will be randomly distributed to indicate a good model fit. In this plot, the residuals appear to be somewhat randomly distributed, although there is a slight pattern indicating potential non-linearity or heteroscedasticity, where variances of residuals are not constant across the range of fitted values.\nNormal Q-Q Plot: This plot helps assess whether the residuals are normally distributed, a key assumption in linear regression. The points in this plot follow the theoretical line closely in the central part but deviate slightly in the tails, suggesting some light deviations from normality, particularly with extreme values, which could be outliers or influential points affecting the regression model.\nScale-Location Plot: This plot also checks for homoscedasticity by plotting the square roots of the standardized residuals against the fitted values. If the variances of residuals are equal across all levels of the independent variables (homoscedasticity), the spread of residuals in this plot should be roughly constant across the range of fitted values. Here, the plot shows a cluster of points with a fairly consistent spread, suggesting that our model meets the assumption of equal variance reasonably well.\nCook’s Distance Plot: This plot identifies influential cases that might overly influence the regression model. Points with a Cook’s distance greater than a critical value (commonly 1 or 0.5) are considered potentially influential. The plot here shows most data points have low Cook’s distances, indicating that no single observation overly influences the overall model fit."
  },
  {
    "objectID": "analysis.html#key-indicators-of-he-factors-in-northeast-region",
    "href": "analysis.html#key-indicators-of-he-factors-in-northeast-region",
    "title": "Analysis",
    "section": "Key Indicators of HE Factors in Northeast Region",
    "text": "Key Indicators of HE Factors in Northeast Region"
  },
  {
    "objectID": "analysis.html#key-indicators-of-he-factors-in-south-region",
    "href": "analysis.html#key-indicators-of-he-factors-in-south-region",
    "title": "Analysis",
    "section": "Key Indicators of HE Factors in South Region",
    "text": "Key Indicators of HE Factors in South Region"
  },
  {
    "objectID": "analysis.html#section-1",
    "href": "analysis.html#section-1",
    "title": "Analysis",
    "section": "",
    "text": "As we take a closer look at the distribution of Key health and Economics indicators. The histograms for the Northeast region and the South region display a similar pattern. Based on the distribution of Key Health and However, there is a significant disparity between those two regions in terms of airborne micro particles factor. In the south region the distribution of this variable is more normal than the northeast region. Indicating the seasonality differences between those regions. On the other hand, the heat exposure distribution of the 2 regions also suggests a significant difference. As we noted, the attainment to health insurance is highly left skewed for both region. This skewed distribution suggests that the variable has a significant impact on the z score. For the green space access, the south region has higher variance than the northeast region suggesting that the green space access may be considered as a more influential factor in northeast region"
  },
  {
    "objectID": "analysis.html#linear-models-for-weighted-average-of-health-and-economic-domain-z-scores-nationally-normed",
    "href": "analysis.html#linear-models-for-weighted-average-of-health-and-economic-domain-z-scores-nationally-normed",
    "title": "Analysis",
    "section": "Linear Models for Weighted Average of Health and Economic Domain z-scores, Nationally Normed",
    "text": "Linear Models for Weighted Average of Health and Economic Domain z-scores, Nationally Normed\n\nResiduals vs Fitted Plot: The plot is used to exam the linearity and homoscedasticity. Based on the graph presented, there is no unequal variance. However, there are some of points that lies outside of the plot, suggesting possible outliers.\nNormal N-Q-Q Plot: This plot is used to test the residuals, as we can see from the graph the N-Q-Q plot for the health and environment domain, most of the cloud point lies on the line, suggesting a normal distribution.\nScale - Location Plot: Despite of some obvious outliers, the plot displays a constant spread across the plot, indicating homoscedasticity. We can see a horizontal line with equally random point, suggesting our model’s robustness.\nCook Distance Plot: For detecting influential observations, we use cook distance plot, as we can see from the plot, the overall cook distance is small indicating that there is no potential strongly influential observations. At the same time, the plot prove the homoscedasticity and no obvious outliers for the health and environment domain."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 7\n\n\nThesis\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 6\n\n\nContinuation of exploratory data analysis\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 5\n\n\nDescription of data set 2\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 4\n\n\nTrends in the Data\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 3\n\n\nData For Equity\n\n\nHow principles for advancing equitable data practice are relevant to our data\n\n\n\n\n\nApr 1, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 2\n\n\nData Background & Data Loading and Cleaning\n\n\nDeeper understanding of dataset & summary and visual of cleaned data\n\n\n\n\n\nMar 18, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nBlog 1\n\n\n3 Datasets\n\n\nSummary of 3 datasets\n\n\n\n\n\nFeb 28, 2024\n\n\nTeam 12\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-04-29-blog-7/blog-7.html",
    "href": "posts/2024-04-29-blog-7/blog-7.html",
    "title": "Blog 7",
    "section": "",
    "text": "Tentative Thesis: The opportunities available to children are determined by various socioeconomic, health and environment, and education factors, especially employment rate, health insurance, and adult education attainment.\nWe have created 3 separate regression models, one for socioeconomic, health and environment, and education. These regression models show the t-statistics and p-values for all the response variables.\nWe decided to base the statistical significance on the t-statistic since there is overlap with the p-values. It was easiest for us to determine the which response variable was most significant by looking at which absolute value was the highest. SE employment rate had a t-statistic of 166.206, HE health insurance had one of 86.873, and ED adult education attainment’s was 106.482. We see that the t-statistics for employment rate, adult education attainment, and health insurance are the highest, which aligns with our thesis.\nAdditionally, we created histograms of the residuals for each of the regression models to visualize their distributions. All 3 histograms show a normal distribution. These are tentative graphics that as of now, simply show the information. We plan on using ggpubr to improve the aesthetics of our graphs and make them easier to understand. Besides the overall looks of our graphics, we plan on writing analyses for each one to highlight the areas we want to focus on and to provide an explanation of the results and how it relates to the project. We were unable to upload the regression models and histograms, which is something we will fix for the future."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-04-08-blog-4/blog-4.html",
    "href": "posts/2024-04-08-blog-4/blog-4.html",
    "title": "Blog 4",
    "section": "",
    "text": "As a team, we have decided to get a more detailed explanation for the trend by exploring its relationship to other variables. In Blog 2, we looked at County and Weighted Average of Social and Economic Domain Component. But to get a better understanding of how County affects the child opportunity levels, we plan to explore the relationship between Education Domain and Health and Environment Domain. This is because by looking at a singular component of the data, we only get narrow understanding of how location can impact opportunity levels. This factors into the depth aspect of data exploration. By focusing on specific areas, we can see how these variables fit and whether other variables should be incorporated.\nWe’ve chosen bar plots to examine the relationship between our area variables (since they are dummy variables), as they effectively illustrate the frequency distribution of each category. Bar plots are particularly useful for comparing the occurrence of different categories, aiding in our understanding of the data’s relationships. Additionally, we will conduct linear regressions for the variables to analyze the relationship between the dependent variable and one or more independent variables by fitting a linear equation to the observed data. For our response variable, we will look at the opportunity levels within the different domains. Our predictor variables will be the different areas (in100).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nindex &lt;- read_csv('scripts/index/index.csv')\n\nRows: 144408 Columns: 37\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): geoid, msaname15, countyfips, statefips, stateusps, c5_ED_nat, c5_...\ndbl (20): year, in100, msaid15, pop, z_ED_nat, z_HE_nat, z_SE_nat, z_COI_nat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlm_model &lt;- lm(r_HE_nat ~ in100, data = index)\nsummary(lm_model)\n\n\nCall:\nlm(formula = r_HE_nat ~ in100, data = index)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.167 -26.451  -2.451  25.549  55.549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  45.1668     0.1469 307.470  &lt; 2e-16 ***\nin100        -0.7162     0.1757  -4.076 4.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.48 on 133805 degrees of freedom\n  (10601 observations deleted due to missingness)\nMultiple R-squared:  0.0001242, Adjusted R-squared:  0.0001167 \nF-statistic: 16.62 on 1 and 133805 DF,  p-value: 4.575e-05\n\n\n\nlibrary(ggplot2)\n\nggplot(index, aes(x = in100)) +\n  geom_bar() +\n  labs(\n    title = \"Distribution of in100\",\n    x = \"in100\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 10601 rows containing non-finite outside the scale range\n(`stat_count()`)."
  },
  {
    "objectID": "posts/2024-02-28-blog-1/blog-1.html",
    "href": "posts/2024-02-28-blog-1/blog-1.html",
    "title": "Blog 1",
    "section": "",
    "text": "#Data set 1:https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/080cfe52-90aa-4925-beaa-90efb04ab7fb\nThe Child Opportunity Index 2.0 database measures neighborhood resources and conditions affecting children’s development and access to opportunities. They used census data from 2015. (Indexes based on the 2010 census are also available.) We will be able to clean the data (though it seems quite clean already). There are 144,408 entries/rows. To better understand the racial demographics of these zip codes, we will need to find the zip codes with the lowest opportunity scores and independently find the racial demographics of these zip codes. Race data is not included in this data set. A question we hope to explore is which of the 29 indicators in the Child Opportunity Index 2.0 database was the greatest predictor of opportunity score? If not measuring based only on one of the indicators, which of the three domains these indicators fall within (education, health and environment, and social and economic) is the best predictor of overall score? Another question we are interested in is based on the 10 zip codes with the lowest composite opportunity scores, and the 10 highest. How do the racial demographics of these two groups differ?\n#Data set 2: https://www.kaggle.com/datasets/rishidamarla/arrests-by-race\n“Arrests by Race” has 19 columns and 31 rows. The rows show the cause for arrest, some being more violent than others. The columns show the total of different racial groups. The data was collected to gain an understanding on number of persons arrested nationwide in 2018 broken down by race and ethnicity. We would be able to load and clean (if needed further cleaning) the dataset to analyze the data. This dataset only shows people arrested by law enforcement that provided their racial demographics. Therefore, it is important to note that the totals for each race may be different than the ethnicity totals. In addition, the data only represents the person arrested and not how many times that person was arrested. A question we hope to explore is are minority races charged with offenses more than White people? Another question is are Black people more likely to get arrested than White people for loitering (or also suspicion) crimes?\n#Data set 3:https://www.census.gov/programs-surveys/ces/data/public-use-data/race-and-economic-opportunity-data-tables.html\nThe data comes from race and economic opportunity. Most previous work on racial disparities has studied inequality within a single generation of people. However, this datasets illustrate the racial gaps among generations. Using de-identified data covering 20 million children and their parents, the data shows how race currently shapes opportunity in the U.S. and how we can reduce racial disparities going forward. Specially, this data illustrates National Statistics by Parent Income Percentile, Gender, and Race. For the analysis of this data, it is necessary for us to interpret and understand the variables. There are 69 variables and we need to identify the dummy variables like gender. For different race, such as Asian, Black, hisp etc. Those data are ranked by income rank. With various kinds of combination, the data can be very messy. We are thinking about filter out the gender and emphasize the impact of different races. There are different outcomes of those children. Going to college, get married or getting in to prison. I believe those are categorical variable and maybe we can convert them into numerical variables to see if there is a correlation between children’s outcome and parents’ incomes and races."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "MA [46]15 Final Project — TEAM 12",
    "section": "",
    "text": "library(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\ncleaned_data &lt;- readRDS('dataset/cleaned_data.rds')\n\nunique_states &lt;- unique(cleaned_data$state_usps)\nlength(unique_states)\n\n[1] 43\n\nregion_data &lt;- data.frame(\n  state_usps = c(\n    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \n    \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n    \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \n    \"WI\", \"WY\", \"DC\"\n  ),\n  region = c(\n    \"South\", \"West\", \"West\", \"South\", \"West\", \"West\", \"Northeast\", \"Northeast\", \"South\", \"South\", \n    \"West\", \"West\", \"Midwest\", \"Midwest\", \"Midwest\", \"Midwest\", \"South\", \"South\", \"Northeast\", \n    \"Northeast\", \"Northeast\", \"Midwest\", \"Midwest\", \"South\", \"Midwest\", \"West\", \"Midwest\", \"Midwest\", \n    \"West\", \"Northeast\", \"Northeast\", \"West\", \"South\", \"Midwest\", \"Midwest\", \"South\", \"Northeast\", \n    \"South\", \"Midwest\", \"Northeast\", \"South\", \"Midwest\", \"South\", \"West\", \"West\", \"Northeast\", \"South\",\n    \"West\", \"Northeast\", \"Northeast\", \"Northeast\"\n  )\n)\n\ncollege_data &lt;- cleaned_data %&gt;%\n  left_join(region_data, by = \"state_usps\") %&gt;%\n  select(aian, api, black, white, hisp, college_enrollment, state_usps, region)\n\ncollege_data_long &lt;- tidyr::pivot_longer(college_data,\n                                         cols = c(aian, api, black, white, hisp),\n                                         names_to = \"race\",\n                                         names_repair = \"unique\")\n\ncollege_data_summarized &lt;- college_data_long %&gt;%\n  group_by(region, race) %&gt;%\n  summarize(average_college_enrollment = mean(value, na.rm = TRUE))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\nggplot(college_data_summarized, aes(x = race, y = average_college_enrollment, fill = race)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~region) +  # Facet by region\n  labs(title = \"Average College Enrollment by Race and Region\",\n       x = \"Race\",\n       y = \"Average College Enrollment\",\n       fill = \"Race\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Pastel1\")\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\ncleaned_data &lt;- readRDS('dataset/cleaned_data.rds')\n\nunique_states &lt;- unique(cleaned_data$state_usps)\nlength(unique_states)\n\n[1] 43\n\nregion_data &lt;- data.frame(\n  state_usps = c(\n    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \n    \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \n    \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \n    \"WI\", \"WY\", \"DC\"\n  ),\n  region = c(\n    \"South\", \"West\", \"West\", \"South\", \"West\", \"West\", \"Northeast\", \"Northeast\", \"South\", \"South\", \n    \"West\", \"West\", \"Midwest\", \"Midwest\", \"Midwest\", \"Midwest\", \"South\", \"South\", \"Northeast\", \n    \"Northeast\", \"Northeast\", \"Midwest\", \"Midwest\", \"South\", \"Midwest\", \"West\", \"Midwest\", \"Midwest\", \n    \"West\", \"Northeast\", \"Northeast\", \"West\", \"South\", \"Midwest\", \"Midwest\", \"South\", \"Northeast\", \n    \"South\", \"Midwest\", \"Northeast\", \"South\", \"Midwest\", \"South\", \"West\", \"West\", \"Northeast\", \"South\",\n    \"West\", \"Northeast\", \"Northeast\", \"Northeast\"\n  )\n)\n\npoverty_rate_data &lt;- cleaned_data %&gt;%\n  left_join(region_data, by = \"state_usps\") %&gt;%\n  select(aian, api, black, white, hisp, poverty_rate, state_usps, region)\n\npoverty_data_long &lt;- tidyr::pivot_longer(poverty_rate_data,\n                                         cols = c(aian, api, black, white, hisp),\n                                         names_to = \"race\",\n                                         names_repair = \"unique\")\n\npoverty_data_summarized &lt;- poverty_data_long %&gt;%\n  group_by(region, race) %&gt;%\n  summarize(average_poverty_rate = mean(poverty_rate, na.rm = TRUE))\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\nggplot(poverty_data_summarized, aes(x = race, y = average_poverty_rate, fill = race)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~region) +  # Facet by region\n  labs(title = \"Average Poverty Rate by Race and Region\",\n       x = \"Race\",\n       y = \"Average Poverty Rate\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Pastel1\")\n\n\n\n\n\n\n\n\n\ncleaned_data &lt;- cleaned_data |&gt;\n  mutate(\n    Region = case_when(\n       state_usps %in% c(\"NJ\", \"NY\", \"NH\", \"PA\", \"VT\", \"MA\", \"ME\", \"CT\", \"RI\") ~ \"Northeast\",\n       state_usps %in% c(\"IL\", \"IN\", \"OH\", \"WI\", \"MI\", \"ND\", \"MN\", \"SD\", \"IA\", \"NE\", \"KS\", \"MO\") ~ \"Midwest\",\n       state_usps %in% c(\"DE\", \"MD\", \"DC\", \"VA\", \"WV\", \"KY\", \"TN\", \"TX\", \"NC\", \"SC\", \n                         \"FL\", \"GA\", \"AL\", \"MS\", \"LA\", \"AK\", \"OK\", \"AR\") ~ \"South\",\n       state_usps %in% c(\"CO\", \"ID\", \"MT\", \"NV\", \"UT\", \"WY\", \"AZ\", \"CA\", \"OR\", \"WA\", \"NM\", \"HI\") ~ \"West\"\n    )\n  )\n\nCOI_mean_scores &lt;- cleaned_data %&gt;%\n  group_by(Region) %&gt;%\n  summarize(mean_z_COI_nat = mean(z_COI_nat, na.rm = TRUE))\n\nggplot(COI_mean_scores, aes(x = Region, y = mean_z_COI_nat, fill = Region)) +\n  geom_col() +  # geom_col is used for bar plots with pre-summarized data\n  labs(title = \"Average COI Z-Scores by Region\",\n       x = \"Region\",\n       y = \"Mean z_COI_nat Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust x-axis labels for better readability"
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-04-18-blog-6/blog-6.html",
    "href": "posts/2024-04-18-blog-6/blog-6.html",
    "title": "Blog 6",
    "section": "",
    "text": "We decided to explore the college enrollment of races by state to further our exploratory data analysis. Since we are analyzing opportunity levels by race for different regions, we thought college enrollment would be a good indicator. This is because college enrollment provides a clear understanding of the earlier education opportunities the children had. For example, if the children were enrolled into colleges, they likely had access to education centers or were enrolled in AP classes. We also are able to assume that they graduated from high school or had proof of a GED. From our graph, we see that White students had the highest college enrollment across the 4 regions, towering over the other race groups (except for Hispanic students in the West). This shows that White students had higher opportunities over other students, but considering this is early in their adult life and only one indicator of opportunity levels, we also wanted to explore their home ownership rate. We found that by graphing the home ownership rate, White students surpassed the other races. We thought home ownership would further our analysis and understanding since it’s a strong measure of how one is doing in life. It shows that the student has a stable income, possibly a family, and is looking to settle down. We plan to repeat this process for various other variables such as median income.\nWe will continue graphing, but we also want to begin creating prediction models. We are curious as to what factors go into college enrollment and the significance of them. For this reason, we will be running regression models to predict college enrollment with independent variables such as median income, AP class enrollment, and access to education centers. Based on the significance of the variables, we will be delving deeper into which race groups and regions have the highest opportunity levels for the significant factors. As of now, this process will be repeated for employment rate and home ownership rate as well. However, based on our analysis, we are flexible to predict other variables using the relevant independent variables."
  },
  {
    "objectID": "posts/2024-04-01-blog-3/blog-3.html",
    "href": "posts/2024-04-01-blog-3/blog-3.html",
    "title": "Blog 3",
    "section": "",
    "text": "Transparency: Be transparent about the limits of the data\nA limitation of data is making analyses that the data does not support. In order to mitigate any unsupported analyses, it is important to be transparent about what data is being used and the process of arriving at the conclusion. Transparency is a relevant limitation to our data set since transparency is a necessity and missing quality in all data sets. One potential for abuse or misuse of our data set is how the data was collected. The data set mainly aims to understand the opportunity levels of children within the 100 largest metros in the US. For this reason, it is possible to make a conclusion that children outside the largest metros have lower health and environment opportunities. However, this would be an unsupported analysis since there is not enough data in the data set to confidently make that conclusion. Another limitation of the data is the z-score measurements. It is unclear what the exact measurements went into calculating the z-score. By not knowing the process of calculating the z-scores, it creates ambiguity and uncertainty when analyzing the score. Urban Institute, as mentioned earlier in this paragraph, emphasizes the importance of being transparent throughout the data collection and analysis process for the reason that it shows validity and credibility. Additionally, by adhering to the practice of being transparent, we can reduce the number of unreliable analyses and more accurately relay our findings to our audience.\n\n\nData Collection: Avoid undue burden\nEnsuring justice in the data collection process is crucial for any data collection. According to the guiding principles for data collection, data collection should reflect respect for the rights of individuals and community groups and minimize undue burden—in other words, eliminate the collection of nonessential elements. Implementing justice in the collection would require a thorough review of the data collection methodology to identify and eliminate unnecessary burdens. It requires regularly reviewing the necessity and usage of collected data elements, ensuring that only relevant data are retained and used. While avoiding undue burdens for the individuals and communities being surveyed is an ethical method for data collection, this also helps maintain cleaner data since it is constantly being checked, and irrelevant data will not be included. Although it is important to adhere to this principle, there are potential limitations to avoiding collecting information deemed too sensitive for certain individuals or groups. It might sound counter-intuitive as the principle asks to prevent unnecessary data collection, but one cannot be entirely sure what data is irrelevant before doing the analysis. Adhering to the justice principle could lead to limitations such as data availability, the potential for certain neighborhoods to be misrepresented, or the index’s inability to capture every nuance of child development environments. The principle itself is an action to reduce abuse and misuse of the data; adhering to this principle would entail not only careful presentation and interpretation of the data but also guidance on its ethical use to inform policy and intervention without reinforcing negative stereotypes or inequalities.\n\n\nAccessability: Return data and research results to community members in a form they can use\nEnsuring that our findings are accessible to people whose data we’ve analyzed is ethically important. After performing our data analysis and finding trends in the data, if valuable, we might share our findings with the creators of the Child Opportunity Index (COI), the Kirwan Institute for the Study of Race and Ethnicity at Ohio State University. This information may also be valuable to nonprofits. Sharing our findings with local governments may also be important, so that assistance programs may be able to specifically target areas with low CO indexes. It would be crucial to publish findings on public or free platforms to promote accessibility."
  },
  {
    "objectID": "posts/2024-03-18-blog-2/blog-2.html",
    "href": "posts/2024-03-18-blog-2/blog-2.html",
    "title": "Blog 2",
    "section": "",
    "text": "Data Background\nThe Child Opportunity Index 2.0 (COI 2.0) is a tool developed by researchers at Brandeis University. The data for COI 2.0 is collected from various state and federal sources, combining information on 29 indicators across three domains: education, health and environment, and social and economic. This meticulous data collection and analysis process is aimed at providing a comprehensive picture of the conditions affecting children’s healthy development across the United States. The COI 2.0 builds on the first version of the Index, created in 2014, and was updated and improved, launching on January 22, 2020. It’s designed to capture a wide range of approximately 72,000 neighborhood conditions like the quality of schools, availability of green spaces, air pollution levels, and the number of adults with high-skill jobs, which are vital for children’s health and development. This tool ranks every U.S. neighborhood and metro area by assigning a Child Opportunity Score from 1 to 100, based on its percentile in the national child opportunity distribution.\nA key goal of the COI 2.0 is to illuminate inequities in neighborhood opportunity within the 100 largest metros in the U.S., and indeed, its first analysis revealed significant racial inequities in neighborhood conditions, which can profoundly impact the health, education, economic prospects, and life expectancy of children. For instance, it found that a majority of black children in Cleveland grow up in very-low and low-opportunity neighborhoods, illustrating a stark pattern of inequity that is replicated across many cities in the nation. The COI 2.0 has been used by researchers, city planners, community leaders, and policymakers to identify current inequities and to create strategies for addressing these disparities. For example, Albany, N.Y. utilized the Index to develop a capital improvement plan to increase access to parks and playgrounds in neglected neighborhoods predominantly inhabited by black children. Similarly, the City of Chicago incorporated the COI into its five-year strategic health plan, Healthy Chicago 2.0, to address child health inequities across the city’s neighborhoods.\n\n\nData Loading and Cleaning\nThe “Child Opportunity Levels, Scores and averaged z-scores for the overall index and three domains” data set is 33mb. For this reason, we did not need to start with a subset of the data. We decided to start the cleaning process by getting rid of the “c5_ED_met”, “c5_HE_met”, “c5_SE_met”, “c5_COI_met”, “r_ED_met”, “r_HE_met”, “r_SE_met”, “r_COI_met”. This is because the majority of values in these columns were left blank. Column “in100” also contains blanks. But after looking at the provided Data Dictionary, we saw that the blanks in this columns are assigned a value and represent whether census tract is located outside a metro or micropolitan area.\nWe decided to plot column z_SE_nat (the weighted average of social and economic domain component) since by summarizing the z_SE_nat, z_ED_nat, z_HE_nat, we saw that z_SE_nat had an abnormal range. The other two columns have a range less than 1, whereas z_SE_nat has a range greater than 1. Upon visualizing z_SE_nat, we also saw that there are a many outliers in the data. While most of the points lie within -1 to 1, there are many that lie outside of that pattern. We will be looking into the relationship between z_SE_nat and countyfips to see if there is any reasoning for the outliers, based on location.\n\nindex &lt;- readRDS('scripts/cleaned_index_dataset.rds')\n\nsummary(index$z_SE_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.806428 -0.123942  0.021308 -0.007073  0.139528  1.888642 \n\nsummary(index$z_ED_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.280814 -0.043922 -0.008535 -0.001110  0.037489  0.256106 \n\nsummary(index$z_HE_nat)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.451968 -0.020446  0.012679  0.006041  0.039472  0.201493 \n\nsuppressPackageStartupMessages(library(tidyverse))\n\nindex %&gt;%\n  ggplot(aes(x = countyfips, y = z_SE_nat)) + \n  geom_point() +\n  labs(\n    title = \"County and Weighted Average of Social and Economic Domain Component\",\n    x = \"County\",\n    y = \"Weighted Average of Social and Economic Domain Component\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/2024-04-12-blog-5/blog-5.html",
    "href": "posts/2024-04-12-blog-5/blog-5.html",
    "title": "Blog 5",
    "section": "",
    "text": "https://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/44ee1ea6-a3df-4af3-93b7-c4695d5ab6a6\nhttps://data.diversitydatakids.org/dataset/coi20-child-opportunity-index-2-0-database/resource/f16fff12-b1e5-4f60-85d3-3a0ededa30a0\nThese will be the two datasets that we will be combining to our original dataset. The first dataset has information about the demographic population in each metropolitan area based on a “geoid”. The second dataset has information about features that play into a child’s overall opportunity, such as whether their parents attended college, whether there is access to healthy foods, and etc, in each metropolitan area. By combining these two datasets with the original one, we can better understand what features contribute to a child’s overall opportunity and further explore how certain features in a metropolitan area affect certain demographics of children. In addition to this, these two datasets allow us to better characterize neighborhoods and understand the disparities and advantages that exist within them.\nWe are still in the middle of figuring out how to combine our datasets together, but we have the general idea that all three datasets have the 8 same variables: ‘geoid’, ‘year’, ‘in100’, ‘msaid15’, ‘msaname15’, ‘countyfips’, ‘statefips’, and ‘stateusps’. Knowing this, we will plan to join these datasets on the ‘geoid’ since this variable is unique for each metropolitan area, leading to a smooth combination. Prior to performing the combination, we will ensure that the data types are the same across all datasets so that the merged dataset remains accurate."
  },
  {
    "objectID": "dataset/testing.html",
    "href": "dataset/testing.html",
    "title": "MA [46]15 Final Project — TEAM 12",
    "section": "",
    "text": "library(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tune         1.2.1 \n✔ infer        1.0.7      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.1.0 \n✔ parsnip      1.2.1      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(broom)\n\nchildpop &lt;- read_csv('dataset/childpop_race.csv', show_col_types = FALSE)\nindicators &lt;- read_csv('dataset/rawindicators.csv', show_col_types = FALSE)\nindex &lt;- read_csv('scripts/index/index.csv', show_col_types = FALSE)\n\n\nchildpop &lt;- childpop |&gt;\n  filter(year != 2010)\n\nindicators &lt;- indicators |&gt;\n  filter(year != 2010)\n\nindex &lt;- index |&gt;\n  filter(year != 2010)\n\n\nmerged_df &lt;- inner_join(childpop, indicators, by = \"geoid\") |&gt;\n  distinct()\ncleaned_merged_df &lt;- inner_join(merged_df, index, by = \"geoid\") |&gt;\n  distinct()\n\n\nlibrary(dplyr)\ncleaned_merged_df &lt;- select(cleaned_merged_df, -year, -in100, -statefips, -stateusps, -pop, -year.y, -msaname15.y, -in100.y, -countyfips.y, -statefips.y, -pop.y, -msaid15.y, -stateusps.y, -countyfips, -pop.x, -msaid15, -msaname15)\n\n\ncleaned_merged_df &lt;- cleaned_merged_df %&gt;%\n  rename(\n    year = year.x,\n    in100 = in100.x,\n    msaid15 = msaid15.x,\n    msaname15 = msaname15.x,\n    county_fips = countyfips.x,\n    state_fips = statefips.x,\n    state_usps = stateusps.x,\n    total_pop = total,\n    AP_enrollment = ED_APENR,\n    adult_edu_attainment = ED_ATTAIN,\n    college_enrollment = ED_COLLEGE,\n    early_child_enrollment = ED_ECENROL,\n    HS_gradrate = ED_HSGRAD,\n    math_prof = ED_MATH,\n    reading_prof = ED_READING,\n    school_pov = ED_SCHPOV,\n    teacher_exp = ED_TEACHXP,\n    edu_centers = ED_PRXECE,\n    high_quality_edu_centers = ED_PRXHQECE,\n    food_access = HE_FOOD,\n    green_space_access = HE_GREEN,\n    heat_exposure = HE_HEAT,\n    health_insurance = HE_HLTHINS,\n    ozone_conc = HE_OZONE,\n    airborne_microparticles = HE_PM25,\n    housing_vac_rate = HE_VACANCY,\n    walkability = HE_WALK,\n    waste_dump_sites = HE_SUPRFND,\n    indus_pollutants = HE_RSEI,\n    poverty_rate = SE_POVRATE,\n    pub_assist_rate = SE_PUBLIC,\n    home_ownership_rate = SE_HOME,\n    high_skill_employ = SE_OCC,\n    med_house_income = SE_MHE,\n    employ_rate = SE_EMPRAT,\n    commute_dur = SE_JOBPROX,\n    single_household = SE_SINGLE\n  )\n\n\nna_rows_count &lt;- sum(!complete.cases(cleaned_merged_df))\ncleaned_data &lt;- na.omit(cleaned_merged_df)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team 12. The members of this team are below."
  },
  {
    "objectID": "about.html#josephine-kim",
    "href": "about.html#josephine-kim",
    "title": "About",
    "section": "Josephine Kim",
    "text": "Josephine Kim\nJosephine is a junior studying Data Science at Boston University."
  },
  {
    "objectID": "about.html#phillip-kim",
    "href": "about.html#phillip-kim",
    "title": "About",
    "section": "Phillip Kim",
    "text": "Phillip Kim\nPhillip is a senior at Boston University majoring in Data Science."
  },
  {
    "objectID": "about.html#zhisheng-yang",
    "href": "about.html#zhisheng-yang",
    "title": "About",
    "section": "Zhisheng Yang",
    "text": "Zhisheng Yang\nZhisheng is a senior studying Economics and Mathematics at Boston University."
  },
  {
    "objectID": "about.html#kajsa-kedefors",
    "href": "about.html#kajsa-kedefors",
    "title": "About",
    "section": "Kajsa Kedefors",
    "text": "Kajsa Kedefors\nKajsa is a senior at Boston University pursuing a joint major in philosophy and psychology, and an independent major in social science and public policy. She minors in journalism."
  },
  {
    "objectID": "about.html#jiahua-chang",
    "href": "about.html#jiahua-chang",
    "title": "About",
    "section": "Jiahua Chang",
    "text": "Jiahua Chang\nJiahua (Richard) is a senior at Boston University majoring in Math and Economics.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Unlocking Childhood Success: The Surprising Key Revealed!",
    "section": "",
    "text": "Where children grow up matters. Their zip code, encompassing factors such as school poverty, the number of early childhood education centers, and the poverty rate plays a big role in children’s future. As does their race, access to health and food, and their median household income.\nOf 29 indicators weighed to predict a scored measure of a child’s access to opportunities, the employment rate of the region the child grows up in is most influential.\nResearchers broke the United States into four regions. The Northeast region has the highest average child opportunity scores and the South has the lowest. Within these regions, the impact of the employment rate significantly stood out.\n\nA high employment rate often indicates a thriving local economy with ample job opportunities, which can translate to better living conditions, access to quality education, and a supportive community environment. Areas with low employment rates may struggle with poverty, limited resources, and fewer opportunities for children to succeed. \nThe employment rate is a key indicator of a region’s economic health, and is closely tied to poverty levels and social well-being. High employment rates are associated with lower poverty rates and better social outcomes, as people have the means to support themselves and their families. More broadly, the employment rate impacts public health, crime rates, and education. Areas with higher employment rates tend to have better access to healthcare and lower crime rates.\nFrom a policy perspective, the employment rate is a crucial metric for assessing the effectiveness of economic policies and interventions. Governments and policymakers often use the employment rate to gauge the success of their efforts to stimulate job growth and improve economic conditions.\n\n\n\nThe distribution of employment rates in the Northeast and South regions are shown in the top left in each of the above figures. Aside from employment rate, median household incomes, shown in the second graph in the middle row, were secondly most associated with a region’s child opportunity scores. This suggests that policies aimed at boosting employment and income levels could help improve the overall economic health of regions like the South, where economic scores are lower.\nThe impact of different indicators is essential for policymakers, educators, and communities to understand to address systemic inequalities and create environments where children in the United States have equal access to opportunities. By investing in education, healthcare, and economic development in underserved communities, and particularly monitoring their employment rate, the United States can help level the playing field and ensure that every child has the chance to reach their full potential, regardless of their zip code or background.\nConclusion: Understanding the impact of these factors is essential for policymakers, educators, and communities to address systemic inequalities and create environments where children have equal access to opportunities. By investing in education, healthcare, and economic development in underserved communities, the United States can help level the playing field and ensure that every child has the chance to reach their full potential, regardless of their zip code or background."
  }
]